{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online learning with Numenta\n",
    "\n",
    "![alt text](https://numenta.org/blog/2013/06/03/images/image.png \"Logo Title Text 1\")\n",
    "\n",
    "## What is Numenta?\n",
    "\n",
    "- Jeff Hawkin's published On Intelligence in 2004. \n",
    "- It was a good layman pop neurosci book for the time, and indirectly led people to deep learning\n",
    "- Andrew Ng for one mentioned it as a strong influence\n",
    "\n",
    "![alt text](https://numenta.com/31d2b7c2eee68b517ffd2184b81db708.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://camo.githubusercontent.com/6ef0e74b2993de2b2e0603e372cfb78312bd358b/687474703a2f2f66657267616c6279726e652e6769746875622e696f2f436c6f727465782d536e617073686f742d3732302e706e67 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/numentaworkshop-gettingstartedoct2014finalv3-150223131414-conversion-gate02/95/getting-started-with-numenta-technology-2-638.jpg?cb=1424714144 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/numentaworkshop-gettingstartedoct2014finalv3-150223131414-conversion-gate02/95/getting-started-with-numenta-technology-13-638.jpg?cb=1424714144 \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "## How is their HTM system different from deep learning?\n",
    "\n",
    "![alt text](http://insidebigdata.com/wp-content/uploads/2016/10/Numenta_figure.png \"Logo Title Text 1\")\n",
    "\n",
    "- HTM uses online unsupervised data (temporal streams), so can't compare to CIFAR results\n",
    "- LSTM is the closest comparison candidate, and it cannot do online learning without scaling very poorly (but this is crucial for AGI)\n",
    "- Lets keep thinking of entirely new architectures. DeepMind'sDQN + moar processing != AGI (backrop isn't the end!)\n",
    "- HTM is directly based on neocortex, DL is inspired by neuroscience\n",
    "- DL systems are very good at doing low-level perceptual tasks, but require huge amounts of examples to learn anything well.\n",
    "- HTM can learn complex temporal structure with several orders of magnitude fewer examples (dozens rather than millions) and can also be easily configured to do reliable one-shot learning\n",
    "\n",
    "![alt text](http://slideplayer.com/slide/4440643/14/images/30/NuPic+Internals:+HTM+Hierarchical+Temporal+Memory.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- HTM networks are not like ConvNets at all. Every aspect is different. Each neuron is more like a network in its own right (using active dendrites), and a layer of neurons uses local k-winner-takes all (inhibition) and not max pooling.\n",
    "- In addition, there is complex structure in using columnar inhibition to represent arbitrary-order sequence memory and to encode prediction errors. \n",
    "- HTM is only really like a convolutional neural network in that it is a tree-like architecture with locally connected (sparse) weights. \n",
    "- Unlike convnets, it doesn't use shared weights. It also serves a different purpose. It acts like a really large predictive autoencoder that predicts its own input one step ahead of time. - From there you can do reinforcement learning and other interesting things. HTM's spatial pooling isn't really like max pooling, it is almost exactly like k-sparse autoencoders though.\n",
    "- HTM is not feed-forward. Information goes both up the hierarchy, to form abstractions and extract features, as well as back down to form prediction \n",
    "\n",
    "### TL;DR: HTM is like a really beefed up autoencoder.\n",
    "\n",
    "### The main thing that interests me in HTM is its online nature. I don't know of any other algorithm that can learn in real-time from data streams without using some form of experience replay. HTM can, and it does so quite well (you can learn a sheet of music, for instance, after as little as 26 iterations).\n",
    "\n",
    "## Apps that use HTM?\n",
    "\n",
    "## Hierarchial Temporal Memory explained\n",
    "\n",
    "\n",
    "### The Neocortex\n",
    "\n",
    "![alt text](https://i.pinimg.com/736x/ca/0c/c0/ca0cc0514e03dd3f384c37599c68ec49--triune-brain-binge-eating.jpg \"Logo Title Text 1\")\n",
    "\n",
    "-reptiles dont have one\n",
    "-mammals do\n",
    "-size of the dinner napkin\n",
    "-older parts of the brain involved in basic functions of life\n",
    "-It makes you \"you\"\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/20141017numentaworkshop-150223125317-conversion-gate02/95/principles-of-hierarchical-temporal-memory-foundations-of-machine-intelligence-5-638.jpg?cb=1424713350 \"Logo Title Text 1\")\n",
    "\n",
    "-Regions linked together in hierarchy\n",
    "-Ideas become more abstract and permanent up the chain\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/20141017numentaworkshop-150223125317-conversion-gate02/95/principles-of-hierarchical-temporal-memory-foundations-of-machine-intelligence-6-638.jpg?cb=1424713350 \"Logo Title Text 1\")\n",
    "\n",
    "- HTM prinicple - Common algorithm everywhere\n",
    "- HTM principle - Sequential memory \n",
    "- HTM princople - Online learning\n",
    "- I/O of neocortex (sensory input and motor commands)\n",
    "- sensory encoder- How to turn input values into SDRs\n",
    "- GPS encoders have no biological counterpart\n",
    "\n",
    "### Sparse Distributed Representations\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/numentaworkshopsubutaisdrtalk-150223132946-conversion-gate02/95/sparse-distributed-representations-our-brains-data-structure-8-638.jpg?cb=1424714054 \"Logo Title Text 1\")\n",
    "\n",
    "- Using 1s and 0s to represents neurons on and off (basic principle of HTM)\n",
    "- The data structure of the brain\n",
    "- Everything is SDRs. used for every aspect of cognitive function\n",
    "- Neurons receive them from other neurons and everywhere\n",
    "- They can overlap i.e sets and unions (combining sdr's')\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/numentaworkshopsubutaisdrtalk-150223132946-conversion-gate02/95/sparse-distributed-representations-our-brains-data-structure-26-638.jpg?cb=1424714054\"Logo Title Text 1\")\n",
    "\n",
    "### Encoders\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/anomalydetectionhtmmeetup-170829200447/95/hierarchical-temporal-memory-for-realtime-anomaly-detection-14-638.jpg?cb=1504037179 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/anomalydetectionhtmmeetup-170829200447/95/hierarchical-temporal-memory-for-realtime-anomaly-detection-27-638.jpg?cb=1504037179 \"Logo Title Text 1\")\n",
    "\n",
    "- like sensory organs like retina or chochlea\n",
    "- outermost system of HTM system\n",
    "- principle 1- Similar data should be highly overlapping\n",
    "- 2- same input should create same output (determinsitic)\n",
    "- 3 output should have same dimensionality as input\n",
    "- 4 output should have similar sparsity as input\n",
    "\n",
    "\n",
    "### Spatial Pooling\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/strangeloophtm-171006200439/95/the-biological-path-towards-strong-ai-strange-loop-2017-st-louis-26-638.jpg?cb=1507320730 \"Logo Title Text 1\")\n",
    "\n",
    "- Accepts input vector and outputs a vector\n",
    "- Important when talking about sequence memory\n",
    "- Maintaining fixed sparsity is a goal\n",
    "- Maintaing overlap properties of inputs for outputs\n",
    "\n",
    "### Temporal Memory\n",
    "\n",
    "- Learns sequences\n",
    "- Predicts outcomes\n",
    "\n",
    "\n",
    "### Towards AGI\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/strangeloophtm-171006200439/95/the-biological-path-towards-strong-ai-strange-loop-2017-st-louis-26-638.jpg?cb=1507320730 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/strangeloophtm-171006200439/95/the-biological-path-towards-strong-ai-strange-loop-2017-st-louis-16-638.jpg?cb=1507320730 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/strangeloophtm-171006200439/95/the-biological-path-towards-strong-ai-strange-loop-2017-st-louis-19-638.jpg?cb=1507320730 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/strangeloophtm-171006200439/95/the-biological-path-towards-strong-ai-strange-loop-2017-st-louis-11-638.jpg?cb=1507320730 \"Logo Title Text 1\")\n",
    "\n",
    "- Weak AI wont produce intelligence\n",
    "- We need to incorporate movement i.e interacting with environment\n",
    "- Better neurons like the pyramidal neuron which has Layers and columns\n",
    "- so the Columns contain layers and layers contain neurons\n",
    "- It has both an ACtivate and inactive state. And predictive state\n",
    "- feedforward, lateral input, and apical (higher level)\n",
    "\n",
    "### is HTM similar to Hinton's Capsule Network?\n",
    "\n",
    "Well for both systems\n",
    "- objects are defined by the relative locations of features\n",
    "- a voting process figures out the most consistent interpretation of sensory data,\n",
    "\n",
    "But the big difference is\n",
    "- HTM models movement. HTM explicitly models how information changes as we move our sensors (e.g. as we move our eyes around), and how to integrate information to quickly recognize objects. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
